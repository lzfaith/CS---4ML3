{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from random import randint\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 1797 data points in total.\nEach data point has 64 features. In fact, each instance is an 8x8 picture of a digit.\nEach digit's target/class/label is one of [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nThe reamining number of data points is 360\n[[ 0.  0.  5. ...  0.  0.  0.]\n [ 0.  0.  0. ... 10.  0.  0.]\n [ 0.  0.  1. ...  3.  0.  0.]\n ...\n [ 0.  0.  0. ... 14.  0.  0.]\n [ 0.  0. 12. ...  2.  0.  0.]\n [ 0.  0.  0. ... 12.  0.  0.]]\n[0 1 0 1 0 1 0 0 1 1 0 0 0 1 1 0 0 0 1 1 1 1 1 0 1 0 0 1 0 1]\n(360, 64)\n"
     ]
    }
   ],
   "source": [
    "# loading the digits data set.\n",
    "digits = load_digits()\n",
    "\n",
    "print (\"We have\", digits.data.shape[0], \"data points in total.\")\n",
    "print (\"Each data point has\", digits.data.shape[1], \"features. In fact, each instance is an 8x8 picture of a digit.\")\n",
    "print (\"Each digit's target/class/label is one of\", list(set(digits.target)))\n",
    "\n",
    "# We want to turn the problem into a binary classification one.\n",
    "# Therefore, from the data set, we only pick those digits whose label is either 0 or 1\n",
    "X = digits.data[(digits.target==0)|(digits.target==1)]\n",
    "Y = digits.target[(digits.target==0)|(digits.target==1)]\n",
    "print (\"The reamining number of data points is\", X.shape[0])\n",
    "print(X[0:30])\n",
    "print(Y[0:30])\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hear are some of the original digits (first three rows)...and then their noisy versions\n"
     ]
    }
   ],
   "source": [
    "def show_some_digits(X, Y):\n",
    "\n",
    "    # Here we just want to visualize some of the data points in the data set, along with their labels. \n",
    "    #\n",
    "    # Create a large figure, to be filled with multiple subplots\n",
    "    plt.figure(figsize=(20,8))\n",
    "    # Go over the first 30 data points\n",
    "    for index, (image, label) in enumerate(zip(X[0:30], Y[0:30])):\n",
    "        plt.subplot(3, 10, index + 1)\n",
    "        # Reshape the 64 features of each data point into an 8x8 image and then plot it.\n",
    "        plt.imshow(np.reshape(image, (8,8)), cmap=plt.cm.gray)\n",
    "        # Write the label of each point.\n",
    "        plt.title('Target Label: %i\\n' % label, fontsize = 10)\n",
    "    return\n",
    "\n",
    "\n",
    "# Here we create a noisy version of the data set\n",
    "# The way that we do it is we go over all the pixels of\n",
    "# each of the data points; then with probability p we multiply\n",
    "# the value of that pixel by 0 (makring it essentially black).\n",
    "# Otherwise (with probability 1-p) we multiply the value of that\n",
    "# pixel by 1 (essentially keeping the pixel untouched)\n",
    "\n",
    "# This the probability p of dropping a pixel (making it zero)\n",
    "def make_it_noisy (X, drop_probability):\n",
    "# This is how we create the noisy data set\n",
    "    return np.multiply(X, np.random.choice([0, 1], size=(360,64), p=[drop_probability, 1 - drop_probability]))\n",
    "\n",
    "\n",
    "print (\"Hear are some of the original digits (first three rows)...and then their noisy versions\")\n",
    "# show_some_digits(X, Y)\n",
    "X_noisy = make_it_noisy (X, 0.6)\n",
    "# show_some_digits(X_noisy, Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define our own nearest neighbor classifier\n",
    "# Any classifier should implement two main functions.\n",
    "# The first one is fit() which takes the training data\n",
    "# and does the training. The second one is predict()\n",
    "# which is used after the training to predict the label\n",
    "# of new points.\n",
    "class MyNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, demo_param='demo'):\n",
    "        self.demo_param = demo_param\n",
    "        return\n",
    "\n",
    "    # For the case of nearest neighbor , we don't really have any training;\n",
    "    # we just need to store the training data points.\n",
    "    def fit(self, X, Y):\n",
    "        # Check that X and y have correct shape\n",
    "        X, Y = check_X_y(X, Y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(Y)\n",
    "        self.X_ = X\n",
    "        self.Y_ = Y\n",
    "\n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self, ['X_', 'Y_'])\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        \n",
    "        # This is the main step of NN which is finding the closest point in the training data\n",
    "        closest = np.argmin(euclidean_distances(X, self.X_), axis=1)\n",
    "        return self.Y_[closest]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our k-nearst neighbor classifer\n",
    "class MyKnearstNeighbor(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, demo_param='demo'):\n",
    "        self.demo_param = demo_param\n",
    "        return \n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        # Check that X and y have correct shape\n",
    "        X, Y = check_X_y(X, Y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(Y)\n",
    "        self.X_ = X\n",
    "        self.Y_ = Y\n",
    "    \n",
    "        self.k = 11\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self, ['X_', 'Y_'])\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "    \n",
    "        Y = []\n",
    "        for x in X:\n",
    "            dist = np.argpartition([i[0] for i in euclidean_distances(self.X_, [x])], self.k)[:self.k]\n",
    "            # Count number of occurrences of each value \n",
    "            bincount = np.bincount([self.Y_[i] for i in dist])\n",
    "            # get the max index\n",
    "            Y.insert(len(Y), np.argmax(bincount))\n",
    "        return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our soft-margin SVM classifer.\n",
    "class MySoftSVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, demo_param='demo'):\n",
    "        self.demo_param = demo_param\n",
    "        return\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        # Check that X and y have correct shape\n",
    "        X, Y = check_X_y(X, Y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(Y)\n",
    "        self.X_ = X\n",
    "        self.Y_ = Y\n",
    "        \n",
    "        # We use Stochastic Gradient Descent (SGD) to train a soft-margin SVM\n",
    "        # The loss function is therefore the hinge loss and\n",
    "        # ... the regularizaiton term will be l2 (Euclidean)\n",
    "        # The constant multiplied by the regulariztion term is alpha (in the class we used lambda)\n",
    "        # max_iter is the maximum number of iterations for the optimization\n",
    "        self.softSVM = SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=1000, alpha=.1)\n",
    "        self.softSVM.fit(X, Y)\n",
    "\n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self, ['X_', 'Y_'])\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        \n",
    "        return self.softSVM.predict(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is our SVC \n",
    "class MyKernelVMClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    def __init__(self, demo_param='demo'):\n",
    "        self.demo_param = demo_param\n",
    "        return\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        # Check that X and y have correct shape\n",
    "        X, Y = check_X_y(X, Y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(Y)\n",
    "        self.X_ = X\n",
    "        self.Y_ = Y\n",
    "        \n",
    "        # the recommand C values: 10, 100, 1000\n",
    "        self.clf = SVC(kernel='rbf', gamma='scale', max_iter=1000, C=1000)\n",
    "        self.clf.fit(X, Y)\n",
    "        return self\n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self, ['X_', 'Y_'])\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        return self.clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus Question\n",
    "class MyBonusQuestion(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, demo_param='demo'):\n",
    "        self.demo_param = demo_param\n",
    "        return\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        # Check that X and y have correct shape\n",
    "        X, Y = check_X_y(X, Y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(Y)\n",
    "        self.X_ = X\n",
    "        self.Y_ = Y\n",
    "        \n",
    "        self.selector = SelectPercentile(f_classif, percentile=10)\n",
    "        self.selector.fit(X, Y)\n",
    "        self.clif = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "        self.clif.fit(self.selector.transform(X), Y)\n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self, ['X_', 'Y_'])\n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        \n",
    "        return self.clif.predict(self.selector.transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 16 23 24 31 32 39 40 47 48 56 57] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 16 23 24 31 32 39 40 47 48 56 57] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 16 23 24 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56 57] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 16 23 24 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56 57] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 16 23 24 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56 57] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 16 23 24 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56 57] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 16 23 24 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56 57] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n  FutureWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 16 23 24 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:114: UserWarning: Features [ 0  7  8 15 23 31 32 39 40 47 48 56 57] are constant.\n  UserWarning)\nD:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py:115: RuntimeWarning: invalid value encountered in true_divide\n  f = msb / msw\n"
     ]
    }
   ],
   "source": [
    "# We want to test the accuracy of SVM, Nearest Neighbor and\n",
    "# ... Logistic Regression classifiers\n",
    "\n",
    "# Our own NN classifier\n",
    "NN = MyNNClassifier()\n",
    "# Our own Soft SVM classifier\n",
    "softSVM = MySoftSVMClassifier()\n",
    "# Logistic Regression directly from scikit library\n",
    "LR = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial')\n",
    "# K-nearest-neighbor classifier \n",
    "kNN = MyKnearstNeighbor()\n",
    "# Our own SVC classifier\n",
    "kernal_version = MyKernelVMClassifier()\n",
    "# Bonus\n",
    "improve_version = MyBonusQuestion()\n",
    "\n",
    "softSVM_scores_all = []\n",
    "NN_scores_all = []\n",
    "LR_scores_all = []\n",
    "kNN_scores_all = []\n",
    "SVC_scores_all = []\n",
    "IV_scores_all = []\n",
    "\n",
    "# We use cross-validation to determine the accuracy.\n",
    "# In t-fold cross-validation, the data is divided into\n",
    "# t subsets. One of the subsets is used for testing, and \n",
    "# the other t-1 subsets (folds) are used for training.\n",
    "# The test score is then calculated. Now, we could have \n",
    "# selected the test subset to be one of the other folds;\n",
    "# therefore, we can have t different possible ways of \n",
    "# computing the socres. We take the average of these t\n",
    "# possible ways as a measure for accuracy of the method.\n",
    "\n",
    "for t in range(2,10):\n",
    "    softSVM_scores = cross_val_score(softSVM, X_noisy, Y, cv=t)\n",
    "    softSVM_scores_all.append(softSVM_scores.mean())    \n",
    "    NN_scores = cross_val_score(NN, X_noisy, Y, cv=t)\n",
    "    NN_scores_all.append(NN_scores.mean())\n",
    "    LR_scores = cross_val_score(LR, X_noisy, Y, cv=t)\n",
    "    LR_scores_all.append(LR_scores.mean())\n",
    "    kNN_scores = cross_val_score(kNN, X_noisy, Y, cv=t)\n",
    "    kNN_scores_all.append(kNN_scores.mean())\n",
    "    SVC_scores = cross_val_score(kernal_version, X_noisy, Y, cv=t)\n",
    "    SVC_scores_all.append(SVC_scores.mean())\n",
    "    IV_scores = cross_val_score(improve_version, X_noisy, Y, cv=t)\n",
    "    IV_scores_all.append(IV_scores.mean())\n",
    "    \n",
    "\n",
    "# Here we plot the cross-validation accuracy of NN and SVM \n",
    "# ... for different number of folds\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(softSVM_scores_all)\n",
    "plt.plot(NN_scores_all)\n",
    "plt.plot(LR_scores_all)\n",
    "# plt.plot(kNN_scores_all)\n",
    "# plt.plot(SVC_scores_all)\n",
    "plt.plot(IV_scores_all)\n",
    "# 'K-nearest-neighbor', 'SVC'\n",
    "plt.legend(['Soft SVM', 'Nearest Neighbor', 'Logistic Regression', 'Bonus'])\n",
    "plt.title('Accuracy of SVM vs NN vs Logistic Regression vs Bonus for different #folds')\n",
    "plt.xlabel('Number of folds')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
